import { Container, IconButton, Text, VStack, Image } from '@yamada-ui/react';
import React, { useContext } from 'react';
import { useEffect, useState, useContext } from 'react';
import RecordRTC from 'recordrtc';
import Header from '../../../components/header/Header';
import startIcon from '/play_circle.svg';
import stopIcon from '/stop_circle.svg';
import micOnIcon from '/mic_on.svg';
import micOffIcon from '/mic_off.svg';
import { context } from '../../../app/App';

function ListenConversationPage() {
  const [transcript, setTranscript] = useState([]);
  const [recorder, setRecorder] = useState(null);
  const [recordings, setRecordings] = useState();
  const [listening, setListening] = useState(false);
  // const [s3Key, setS3Key] = useState('');
  const { BASE_URL } = useContext(context);

  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  const recognition = new SpeechRecognition();

  // Ë™çË≠ò„ÅåÈñãÂßã„Åï„Çå„Çã„Åü„Å≥„Å´ÈÄ£Á∂ö„Åó„ÅüÁµêÊûú„Çí„Ç≠„É£„Éó„ÉÅ„É£
  recognition.continuous = true;
  // Èü≥Â£∞Ë™çË≠ò„Ç∑„Çπ„ÉÜ„É†„Åå‰∏≠ÈñìÁöÑ„Å™ÁµêÊûú„ÇíËøî„Åô„ÄÄÊúÄÁµÇÁöÑ„Å™ÁµêÊûú„Å†„ÅëËøî„ÅôÂ†¥Âêà„ÅØfalse
  recognition.interimResults = true;
  recognition.lang = 'ja-JP'; // Êó•Êú¨Ë™ûÂØæÂøú

  const onStart = () => {
    setTranscript('');
    setListening(true);
    recognition.start();
  };

  const onStop = () => {
    recognition.stop();
    setTranscript('');
    setListening(false);
  };

  useEffect(() => {
    return () => {
      if (recorder) {
        recorder.destroy();
      }
    };
  }, [recorder]);

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: true,
        video: false,
      });
      const newRecorder = new RecordRTC(stream, { type: 'audio' });
      newRecorder.startRecording();
      setRecorder(newRecorder);

      onStart();
    } catch (error) {
      if (error) {
        console.log(`Èå≤Èü≥„ÅÆÈñãÂßã„Å´Â§±Êïó„Åó„Åæ„Åó„ÅüÔºö${error.message}`);
      }
    }
  };

  const stopRecording = () => {
    if (recorder) {
      recorder.stopRecording(() => {
        const blob = recorder.getBlob();
        onStop();
        const newRecording = blob;
        setRecordings(newRecording);
      });
    }
  };

  useEffect(() => {
    if (recordings) {
      (async () => {
        const mp3File = Math.random().toString(32).substring(2) + new Date().getTime().toString(32);
        await post(mp3File);
        await text(mp3File);
      })();
    }
  }, [recordings]);

  const post = async (mp3File) => {
    console.log(recordings);
    if (recordings) {
      const formData = new FormData();
      formData.append('audio', recordings);

      const mp3Blob = await fetch(`${BASE_URL}/api/voices/convert`, {
        method: 'POST',
        body: formData,
      }).then((res) => res.blob());

      const uploadForm = new FormData();

      // Ôºì„Å§ÁõÆ„ÅÆÂºïÊï∞„ÅØ„Éï„Ç°„Ç§„É´Âêç„Å®„Åó„Å¶ÊåáÂÆö„Åï„Çå„Çã
      uploadForm.append('audio', mp3Blob, `${mp3File}.mp3`);

      const uploadRes = await fetch(`${BASE_URL}/api/voices/upload`, {
        method: 'POST',
        body: uploadForm,
      }).then((res) => res.json());
      // setS3Key(uploadRes.key);
    }
  };

  const text = async (mp3File) => {
    const data = await fetch(`${BASE_URL}/api/voices/transcription-result/${mp3File}`).then((res) =>
      res.json(),
    );

    console.log('üçì ~ text ~ data:', data.status);
    console.log('üçì ~ text ~ data.text:', data.text);
    if (data.status === 'completed') {
      setTranscript([data.text]);
    } else if (data.status === 'in_progress') {
      setTimeout(async () => await text(mp3File), 5000);
    } else {
      console.error('ÊñáÂ≠óËµ∑„Åì„Åó„Å´Â§±ÊïóÔºö', data.reason);
    }
  };

  return (
    <>
      <Container
        centerContent="true"
        gap="none"
        p="0">
        <Header title={'„Åµ„Åü„ÇäÂØæË©±'} />
        <Container
          marginTop="60px"
          paddingTop="60px">
          <VStack alignItems="center">
            <Text
              color="tertiary"
              fontSize="23px"
              padding="md">
              ‰ºöË©±„Çí„Å≤„Çç„ÅÜ
            </Text>
            {listening ? (
              <VStack
                align="center"
                gap="60px">
                <Image
                  src={micOnIcon}
                  alt="„Éû„Ç§„ÇØon"
                  boxSize="246px"
                />
                <IconButton
                  icon={
                    <img
                      src={stopIcon}
                      alt="Èå≤Èü≥ÂÅúÊ≠¢"
                    />
                  }
                  onClick={stopRecording}
                  size="auto"
                  width="87px"
                  alignItems="center"
                  variant="primary"></IconButton>
              </VStack>
            ) : (
              <VStack
                align="center"
                gap="100px">
                <Image
                  marginTop="44px"
                  src={micOffIcon}
                  alt="„Éû„Ç§„ÇØoff"
                  boxSize="162px"
                />
                <IconButton
                  icon={
                    <img
                      src={startIcon}
                      alt="Èå≤Èü≥ÈñãÂßã"
                    />
                  }
                  onClick={startRecording}
                  size="auto"
                  width="87px"
                  alignItems="center"
                  variant="primary"></IconButton>
              </VStack>
            )}
          </VStack>

          {transcript ? (
            transcript.map((elem) => {
              return (
                <div key={elem.id}>
                  {elem.speaker_lavel}
                  {elem.transcript}
                </div>
              );
            })
          ) : (
            <></>
          )}
        </Container>
      </Container>
    </>
  );
}

export default ListenConversationPage;
